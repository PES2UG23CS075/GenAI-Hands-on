{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Assignment 1: The Movie Age Calculator (LCEL)\n",
        "**Objective**\n",
        "\n",
        "Create a functional chain using the pipe operator (|) that takes a movie title, identifies its release year, and calculates how many years have passed since then.\n",
        "\n",
        "Key Concepts Used\n",
        "LCEL (LangChain Expression Language): Linking components in a single line.\n",
        "\n",
        "Prompt Templates: Using placeholders like {movie_name} for dynamic input.\n",
        "\n",
        "Output Parsers: Using StrOutputParser to get clean text instead of a complex object.\n"
      ],
      "metadata": {
        "id": "3-46g3gjUwyG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-3Rq53SKFPvp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ba0fefc-c0e3-4219-c8e8-24403f7b34ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/66.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install python-dotenv --upgrade --quiet langchain langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 1. Securely load your API Key\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")\n",
        "\n",
        "# 2. Initialize the Model\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "\n",
        "# 3. Create the Prompt Template (This asks for the release year AND the math!)\n",
        "template = ChatPromptTemplate.from_template(\n",
        "    \"What year was the movie '{movie_name}' released? Please calculate how many years ago that was from the current year.\"\n",
        ")\n",
        "\n",
        "# 4. Initialize the Output Parser (To clean up the response into a readable string)\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# 5. THE ASSIGNMENT: Build the chain in one line using LCEL\n",
        "chain = template | llm | parser\n",
        "\n",
        "# 6. Test it out!\n",
        "response = chain.invoke({\"movie_name\": \"Inception\"})\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RWVuMRdTAaY",
        "outputId": "109d5df8-5823-417d-aeb9-baaffa09e00a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Google API Key: ··········\n",
            "The movie 'Inception' was released in **2010**.\n",
            "\n",
            "That was **14 years ago** from the current year (2024).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# 1. Advanced Template with Roles\n",
        "# Using roles (System vs Human) is the 'safe' and professional way to handle LLMs.\n",
        "advanced_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a cynical film critic. You provide the release year and the math, but you always add a witty, slightly judgmental comment about the movie.\"),\n",
        "    (\"human\", \"Tell me about the movie '{movie_name}'.\")\n",
        "])\n",
        "\n",
        "# 2. Re-defining the chain\n",
        "chain = advanced_template | llm | parser\n",
        "\n",
        "# 3. Streaming the output (The \"Cool\" way to show data)\n",
        "# LCEL allows you to stream the response word-by-word instead of waiting for the whole thing!\n",
        "for chunk in chain.stream({\"movie_name\": \"Titanic\"}):\n",
        "    print(chunk, end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03GjpF40THPy",
        "outputId": "0ebaa471-ed34-44e3-e2fe-3641b1b03ce3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ah, *Titanic*.\n",
            "\n",
            "That cinematic behemoth crashed onto screens in **1997**. Do the math, and that's **27 years ago**—a quarter-century since we all collectively agreed that Jack absolutely could have fit on that door. A testament to how much people enjoy watching beautiful rich people suffer, preferably in extremely drawn-out fashion."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WOQ6QmuITwLa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}